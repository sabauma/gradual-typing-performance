Notes on iterations
===

Just read about & implemented a protocol for running trials (here).
Instead of doing 30 iterations, we now:

1. Run 10 iterations
2. Use the Anderson-Darling test to check if the data is obviously not normal
    (Currently using 1% critical value; in English, "if the data came from
     a normal dist., there is a 1% chance of seeing these numbers")
3. If not-normal, run 4 more iterations & check step 2, up to a max of 30 iterations
4. Otherwise, stop early

Caveats:
- Assumes our samples are normally distributed
- Assumes each trial is independent
  (note: we ignore a warm-up run)
- We accept 30 iters, no matter how weird the data is
- We do not try to filter outliers

But this should be good enough for our measurements.
We just want to save time if we got a bunch of stable runs.

(I'll write this up in detail in the journal paper.)
