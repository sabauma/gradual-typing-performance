This is not your usual POPL paper, please read carefully. 

 GT=SOUND Gradual Typing 
 TR=Typed Racket 
JIT=Just in time compiler 


Are our results already known in light of previous work?
********************************************************

SOUND GT follows from a distinguished series of foundational works:

 SFP 2006 and DLS 2006
 ECOOP 2007, ESOP 2009
 POPL 2008, 2010, 2011, 2x 2015
 PLDI 2015

This paper is the first to present a thorough EVALUATION of GT. While we
used the idea in our OWN ECOOP paper for TWO SMALL BENCHMARKS, nothing in
that paper explains/motivates/validates the framework.

The experimental results are for a mature implementation of GT (with a user
base) on programs representative of idiomatic usage of TR.

Our results identify a FUNDAMENTAL CHALLENGE to the ENTIRE LINE OF WORK.
They are a call to arms for designers and implementers.


Are our results of limited applicability?
*****************************************

* #81A asks if choosing TR biases our experiments and endangers our conclusions.

TR is representative of an approach to GT that goes back to the 2 Ur-papers
from 2006.  Any language that tags values with the casts applied to them
behaves similarly to TR: there will be allocations when the casts are
applied; indirections to access data; and checks at use points. TR's
implementation is NOT a naive implementation of GT.  It includes a
traditional JIT and many optimizations.

* #81B claims the impossibility of a negative result without evaluating the
* potential benefits of JITs.

The primary result is the EVAL FRAMEWORK. The negative evaluations for the
most mature implementation of GT is a VALIDATION OF ITS USEFULNESS.

What impact MIGHT hypothetical GT-aware optimizations have on performance?
The cost of GT split in three:

-- cast: requires allocation
-- unprotected access: may be slower as any value may have a contract
-- protected access: checks must be run

There is NO LOCALITY the JIT can leverage. A value cast in one part of the
program flows across module boundaries to arbitrary destinations.  There are
NO EXISTING optimizations that would avoid these costs in the presence of
dynamic features.

* #81A laments the lack of attention of micro-level gradual typing.

We tried! There are no robust implementations of GT with rich libraries.

* #81A asks about the performance of TypeScript.

TypeScript is UNSOUND!  The underlying JavaScript VM sees the same code for
each configuration in the lattice.

StrongScript (by Vitek) has low overheads but it remains UNSOUND.

* #81B ask about an evaluation using Safe TypeScript programs.

We have not done this. We note that POPL'15 reported a 77x slowdown for a
fully dynamic program, so it is conceivable that an application of our
framework would yield similarly frightening results.

Our paper is a challenge to other implementors of GT to evaluate their work
properly before flooding POPL with more theoretical papers.

* #81B states that N-deliverability, N/M-usability are unhelpful as the
* performance of gradual typing is so bad that there is no point to them.

* #81B also states that 50% overhead is likely the maximum acceptable.

The very reason for this formulation is that any user can pick a threshold.
If N=50 is what is required then clearly GT in its present form is dead.  No
JIT optimizations will save us.

Our point is that if someone claims to have a solution that speeds up GT,
this is the framework to evaluate that solution.



* More details on costs and potential solutions?
************************************************

* #81A asks for constructive suggestions and solutions.
* #81B+#81A ask for details about what kinds of checks are expensive.


TR/GT is a +10-year research investment. The magnitude of the overheads and
the ease with which a programmer can fall off a performance cliff came as a
SHOCK to us.  This paper's role is to stand as a warning to others in the
field, and to provide a methodology they can follow to evaluate their
implementations.

We have results on specific bottlenecks which we will add -- with 
permission from the PC chair.

================================================================================
==== End of 500 word limit =====================================================
================================================================================

Detailed response to Reviewer #81C 
**********************************

* Who wote the programs that you study?

All but 2 programs are pre-existing and idiomatic:

              *** = Written for this paper

gregor     -- Jon Zeppieri (library, untyped)
kcfa       -- Matt Might, Jay McCarthy (teaching, untyped)
lnm        -- ***  (app)
mbta       -- Matthias Felleisen (courseware, untyped)
morse-code -- John Clements, Neil Van Dyke (app+library, untyped)
quad       -- Matthew Butterick (app, untyped+typed)
sieve      -- *** (artificial)
snake      -- David Van Horn, Phil Nguyen & Sam Tobin-Hochstadt (game, untyped)
suffixtree -- Danny Yoo (library, untyped)
synth      -- Vincent St-Amour, Neil Toronto (app+library, typed)
tetris     -- David Van Horn, Phil Nguyen, Sam Tobin-Hochstadt (game, untyped)
zo         -- Ben Greenman (library, untyped)


* You talk about "adapting" existing programs.

Adaptations were the addition/removal of type annotations for benchmarking.

* When typing a module in TR, is there just one choice of typing?

We chose the most specific types where possible. We will clarify.

* The programs you study do NOT require teams of implementers.

Yes, we will clarify.

* The "Gregor" benchmark sounds synthetic.

The library is used in real-world programs.

* Your L/N/M metrics ignore the size of modules and their ease of typability.

Annotations are required on definitions (structures, functions) so their
number is not directly proportional to the size of a module in LOC. 
We will clarify.

* I got confused by your use of "clique".

Yes! We used a technical term from algorithms with its loose, colloquial
meaning to apply to a measure of software complexity. We will clarify.

* quad missing from Figure 5

We have data for quad now. It also looks bad.

* "Our results may be less valid in the context of large programs, though
*  practical experience using TR suggests otherwise."  ???

We will elaborate.

* To what extent can type inference help in inferring types for modules?

This is an interesting idea, we will consider it. Thanks!

* It's cool to have done exhaustive consideration of typed configurations,
* but, as you remark, this is not feasible for larger programs. Why not
* choose random paths?

Random walks is one of the many, many things we experimented with while
doing this research.  For this paper, we wanted to give an exhaustive view
of the space of configurations to convince the readers that we were not
cherry-picking bad ones. In short, the evaluation framework reveals
fundamental weaknesses that were completely overlooked in POPL so far.
